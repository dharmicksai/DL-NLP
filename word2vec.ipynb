{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMIjoa91dtj5640yEi/CYvK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dharmicksai/DL-NLP/blob/main/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQm3PK5Ewirs"
      },
      "source": [
        "install fasttext and sentence transformer packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-vlUhlL4RHU",
        "outputId": "3846a422-6338-4cd2-cd8e-dbfe6af4beb6"
      },
      "source": [
        "!pip install fastText"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastText in /usr/local/lib/python3.7/dist-packages (0.9.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fastText) (1.19.5)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fastText) (56.0.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fastText) (2.6.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhTeLjChofix",
        "outputId": "35d1fc3c-725e-4be7-99e0-aee308d3c451"
      },
      "source": [
        "!pip install sentence_transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.2.5)\n",
            "Requirement already satisfied: transformers<5.0.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.5.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.1.95)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.8.1+cu101)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.15.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (3.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (20.9)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (0.10.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence_transformers) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence_transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence_transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR7WDeVf49tR",
        "outputId": "5cea2212-2cb0-4e5c-dfbb-a78e64f7d4be"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords') #download stop words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gkr3bczthJuk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6c12c1b-7aa0-4ab1-f93d-b064c457afae"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "import time\n",
        "import tqdm.autonotebook"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VG2173R-Iw-",
        "outputId": "b7a792e9-da2c-4734-cf94-53c673c99f0a"
      },
      "source": [
        "import fasttext.util\n",
        "fasttext.util.download_model('en', if_exists='ignore')  # English\n",
        "ft = fasttext.load_model('cc.en.300.bin') #load fastext model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i77mB22tcO-w",
        "outputId": "15a86f3e-7bab-4e64-a76c-c91061788fa3"
      },
      "source": [
        "!wget -c http://nlp.stanford.edu/data/glove.840B.300d.zip #download glove dictionary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-07 08:54:17--  http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.840B.300d.zip [following]\n",
            "--2021-05-07 08:54:17--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [following]\n",
            "--2021-05-07 08:54:17--  http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQegscRLcb8V"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/glove.840B.300d.zip\", \"r\") as zipread: #extract zip files\n",
        "  zipread.extractall(\"/content/\")\n",
        "  zipread.close"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5m6sDQWbZkS"
      },
      "source": [
        "import gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EkEf8MQcxKm"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "PATH_TO_GLOVE = \"/content/glove.840B.300d.txt\"\n",
        "\n",
        "tmp_file = \"/glove.840B.300d.w2v.txt\"\n",
        "glove2word2vec(PATH_TO_GLOVE, tmp_file)\n",
        "glove = gensim.models.KeyedVectors.load_word2vec_format(tmp_file) #load glove model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlWRQM_ghqoW"
      },
      "source": [
        "data = pd.read_csv(\"data.csv\") #read data file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbeH8tdIiGbI"
      },
      "source": [
        "data.dropna(subset=['Extract from Book/Article'])\n",
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "TivheYFMiWX-",
        "outputId": "61ebab91-2f39-47e8-9f89-d0738bc87009"
      },
      "source": [
        "data.loc[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Next time you’re faced with a problem with many possible answers, pinpoint your end goals and then come up with a solution for each. This is likely to lead to the generation of a diverse set of options covering multiple categories of solutions.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsKNUGvxyZAA"
      },
      "source": [
        "create dictionary of extracts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAAvH39xkvPS"
      },
      "source": [
        "sentences = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYNfJ_qRltwX"
      },
      "source": [
        "for row_index,row in data.iterrows():\n",
        "   sentences[row_index] = str(row[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEfw_L-tpJ7I",
        "outputId": "5d1c631b-7f60-4f64-efbf-c28dc2aa0a77"
      },
      "source": [
        "print(len(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN9MhNXCyppX"
      },
      "source": [
        "Select E1 => 30 words , E2 => 60 words and E3 => 100 words extracts <br>\n",
        "store query ids as e1 , e2 , e3<br>\n",
        "and remove them from database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0LZNeqroBjy",
        "outputId": "a9ca1409-99eb-4681-cc33-feca71ae9dbd"
      },
      "source": [
        "E1 = \"\"\n",
        "e1 = 0\n",
        "for key in sentences:\n",
        "  y = sentences[key]\n",
        "  # print(y)\n",
        "  if abs(len(y.split())-30) == 0 : #check num words =30\n",
        "    E1 = str(y)\n",
        "    e1 = key\n",
        "    sentences.pop(key)\n",
        "    break\n",
        "print(E1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maintenance takes longer, and that stifles innovation. Toilsome work leads to burnout, which leads to attrition. That, in turn, leads to lost organizational knowledge, which exacerbates maintenance, onboarding, and innovation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqd9z0P9s2Mq",
        "outputId": "f823360e-37b9-44e5-f966-818ead5f27e8"
      },
      "source": [
        "E2 = \"\"\n",
        "e2 = 0;\n",
        "for key in sentences:\n",
        "  y = sentences[key]\n",
        "  if abs(len(y.split())-60) == 0: #check num words =60\n",
        "    E2 = str(y)\n",
        "    e2 = key\n",
        "    sentences.pop(e2)\n",
        "    break\n",
        "print(E2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our psyches, like our bodies, can become a bit old and stiff and ossified. We can get stuck in a rut of thinking, and the world starts to seem a boring and stale place. We can practice getting out of ‘old fogey mind’ and back into ‘beginner’s mind’, where we are open, fluid, playful and curious to what is happening. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUfj1oZEtDb_",
        "outputId": "dd8011ee-becf-4fa9-9da0-b81bb2e953f2"
      },
      "source": [
        "E3 = \"\"\n",
        "e3 = 0\n",
        "for key  in sentences:\n",
        "  y = sentences[key]\n",
        "  if abs(len(y.split())-100) == 0: #check num words = 100\n",
        "    E3 = str(y)\n",
        "    e3 = key\n",
        "    sentences.pop(e3)\n",
        "    break\n",
        "\n",
        "print(E3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What you still need to know is this: before a dream is realized, the Soul of the World tests everything that was learned along the way. It does this not because it is evil, but so that we can, in addition to realizing our dreams, master the lessons we’ve learned as we’ve moved toward that dream. That’s the point at which most people give up. It’s the point at which, as we say in the language of the desert, one ‘dies of thirst just when the palm trees have appeared on the horizon.” “Every search begins with beginner’s luck. And every search ends with the victor’s being severely tested.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwWaWVgQ61SS",
        "outputId": "2c86221e-4b30-4885-a7e4-b113a86dc926"
      },
      "source": [
        "print(len(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "814\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUvsvdZ-peSF"
      },
      "source": [
        "queries = [E1,E2,E3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9ZuhVD3bSow"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "import os\n",
        "import string\n",
        "import gensim\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1yq7O8Il0DZ"
      },
      "source": [
        "import scipy\n",
        "from ast import literal_eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl6etqxuzg7L"
      },
      "source": [
        "function to return mean of glove  word2vec embeddings of words in a extract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilcrvN3sc5QU"
      },
      "source": [
        "def glove_wordembedding_method(s1):\n",
        "    vector_1 = np.mean([glove[word] for word in s1.split() if word in glove],axis=0)\n",
        "    return vector_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_YUoFOmndTY"
      },
      "source": [
        "corpus_embeddings = [] # embeddings of each extract in corpus\n",
        "for key in sentences:\n",
        "  y = sentences[key]\n",
        "  corpus_embeddings.append(glove_wordembedding_method(y))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr0UX_V6iEwg"
      },
      "source": [
        "corpus = [] # all extracts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm_cgWariOgQ"
      },
      "source": [
        "keys = [] # ids of extracts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5xFKvZWiHnh"
      },
      "source": [
        "for key in sentences:\n",
        "  y = sentences[key]\n",
        "  corpus.append(y)\n",
        "  keys.append(key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QqSCYYRhTxs",
        "outputId": "a1a24729-9f95-4d6a-bfb1-1d4db987f1be"
      },
      "source": [
        "len(corpus_embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "814"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WqWbqkJq_1I"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL19vKtUh2Bz",
        "outputId": "4aa4376c-c348-45e1-abb8-32b650f8909b"
      },
      "source": [
        "print(e1,e2,e3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "266 298 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYuNwfgj1GaZ"
      },
      "source": [
        "glove similar sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMYW3jZ4pJ2I"
      },
      "source": [
        "from sentence_transformers import util\n",
        "que_id = [] # id of query\n",
        "que = [] #query\n",
        "similar_sentences = [] #similar sentence\n",
        "scores = [] #similarity scores\n",
        "corpus_ids = [] #ids of corpus\n",
        "for i in range(len(queries)): # for each querieies\n",
        "  query = queries[i]\n",
        "  q_e = glove_wordembedding_method(query) #embedding of query\n",
        "  q_e = torch.tensor(q_e)\n",
        "  cos_scores = util.pytorch_cos_sim(q_e, corpus_embeddings)[0] #cosine scores and ids\n",
        "  top_results = torch.topk(cos_scores, k=10) #function for finding top 10 scores\n",
        "  for score, idx in zip(top_results[0], top_results[1]):\n",
        "        # addd results to lists\n",
        "        que.append(query)\n",
        "        similar_sentences.append(corpus[int(idx)])\n",
        "        scores.append(score)\n",
        "        corpus_ids.append(keys[int(idx)])\n",
        "        if i==0:\n",
        "          que_id.append(e1)\n",
        "        if i==1:\n",
        "          que_id.append(e2)\n",
        "        if i==2:\n",
        "          que_id.append(e3)\n",
        "data = {\"que_id\":que_id,\"query\":que, \"similar_sentences\":similar_sentences , \"scores\":scores , \"corpus_ids\" : corpus_ids}\n",
        "df = pd.DataFrame(data)\n",
        "#save as csv file\n",
        "df.to_csv('cosine_similarity_Glove.csv')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjdT0F1wjh0w"
      },
      "source": [
        "frequencies={} # frequency of each word\n",
        "for key in sentences:\n",
        "  y = sentences[key]\n",
        "  for word in y.split():\n",
        "    if word in frequencies:\n",
        "      frequencies[word]+=1\n",
        "    else:\n",
        "      frequencies[word] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ8dsu8dkKOV"
      },
      "source": [
        "total_freq = sum(frequencies.values())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1X-IuUN3C_r"
      },
      "source": [
        "function to return weighted average of glove  word2vec embeddings of words in a extract , weight of word is inversly proportional to its frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mJUmIYwjkBz"
      },
      "source": [
        "def glove_Invers_freq_wordembedding_method(s1):\n",
        "    a=0.001\n",
        "    weight = [a/(a+frequencies.get(word, 0)/total_freq) for word in s1.split() if word in glove] # weight of each word embeding is total_freq/wordfreq\n",
        "    vector_1 = np.average([glove[word] for word in s1.split() if word in glove],axis=0,weights=weight) #weighted average\n",
        "    return vector_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLTNuPiW2rT-"
      },
      "source": [
        "glove Inverse frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSKKTriVk4ir"
      },
      "source": [
        "corpus_embeddings = [] # corpus embeddings\n",
        "for key in sentences:\n",
        "  y = sentences[key]\n",
        "  corpus_embeddings.append(glove_Invers_freq_wordembedding_method(y))\n",
        "que_id = [] #query id\n",
        "que = [] # query\n",
        "similar_sentences = [] #similar sentences\n",
        "scores = [] #scores\n",
        "corpus_ids = [] #corpus ids\n",
        "for i in range(len(queries)): # for each query\n",
        "  query = queries[i]\n",
        "  q_e = glove_Invers_freq_wordembedding_method(query)\n",
        "  q_e = torch.tensor(q_e)\n",
        "  cos_scores = util.pytorch_cos_sim(q_e, corpus_embeddings)[0] # cosine similarity scores\n",
        "  top_results = torch.topk(cos_scores, k=10) #fetch top 10 results\n",
        "  for score, idx in zip(top_results[0], top_results[1]):\n",
        "        # append results to lists\n",
        "        que.append(query)\n",
        "        similar_sentences.append(corpus[int(idx)])\n",
        "        scores.append(score)\n",
        "        corpus_ids.append(keys[int(idx)])\n",
        "        if i==0:\n",
        "          que_id.append(e1)\n",
        "        if i==1:\n",
        "          que_id.append(e2)\n",
        "        if i==2:\n",
        "          que_id.append(e3)\n",
        "data = {\"query\":que, \"similar_sentences\":similar_sentences , \"scores\":scores , \"corpus_ids\" : corpus_ids}\n",
        "df = pd.DataFrame(data)\n",
        "#save as csv files\n",
        "df.to_csv('cosine_similarity_Glove_Invers_freq.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6WvuFLP4IlX"
      },
      "source": [
        "Removing stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbloG2sq4_rG"
      },
      "source": [
        "\n",
        "filtered_sentences = []\n",
        "filtered_quiries = []\n",
        "stop_words = set(stopwords.words('english')) # set of stop words\n",
        "for key in sentences: # for each extract\n",
        "  y = sentences[key]\n",
        "  filtered_sentence = []\n",
        "  for w in y.split(): \n",
        "      if w not in stop_words: #check if word is not a stop word\n",
        "          filtered_sentence.append(w)\n",
        "  filtered_sentences.append(filtered_sentence)\n",
        "\n",
        "for query in queries:\n",
        "  y = query\n",
        "  filtered_query = []\n",
        "  for w in y.split(): \n",
        "      if w not in stop_words: #check if word is not a stop word\n",
        "          filtered_query.append(w)\n",
        "  filtered_quiries.append(filtered_query)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLGWt-l94wcJ"
      },
      "source": [
        "function for glove no stop words embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlL7cU6b77E5"
      },
      "source": [
        "def glove_no_stop_wordembedding_method(s1):\n",
        "    vector_1 = np.mean([glove[word] for word in s1 if word in glove],axis=0) #mean of word embeddings\n",
        "    return vector_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRTtEvsbFqNd"
      },
      "source": [
        "glove no stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnlzwI_j7f9o"
      },
      "source": [
        "corpus_embeddings = []\n",
        "for y in filtered_sentences: #glove embeddings without stop words\n",
        "  corpus_embeddings.append(glove_no_stop_wordembedding_method(y))\n",
        "que_id = []\n",
        "que = []\n",
        "similar_sentences = []\n",
        "scores = []\n",
        "corpus_ids = []\n",
        "for i in range(len(filtered_quiries)): # for each query\n",
        "  query = filtered_quiries[i]\n",
        "  q_e = glove_no_stop_wordembedding_method(query) # query embedding\n",
        "  q_e = torch.tensor(q_e)\n",
        "  cos_scores = util.pytorch_cos_sim(q_e, corpus_embeddings)[0] #cosine scores\n",
        "  top_results = torch.topk(cos_scores, k=10) #find top 10\n",
        "  for score, idx in zip(top_results[0], top_results[1]):\n",
        "        #append results\n",
        "        que.append(query)\n",
        "        similar_sentences.append(corpus[int(idx)])\n",
        "        scores.append(score)\n",
        "        corpus_ids.append(keys[int(idx)])\n",
        "        if i==0:\n",
        "          que_id.append(e1)\n",
        "        if i==1:\n",
        "          que_id.append(e2)\n",
        "        if i==2:\n",
        "          que_id.append(e3)\n",
        "data = {\"query\":que, \"similar_sentences\":similar_sentences , \"scores\":scores , \"corpus_ids\" : corpus_ids}\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('cosine_similarity_Glove_no_stop_words.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFeyu_yx5ve7"
      },
      "source": [
        "function for mean fasttext word2vec embeddings of words in extract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23YgISmG-MsI"
      },
      "source": [
        "def fasttext_wordembedding_method(s1):\n",
        "    vector_1 = np.mean([ft.get_word_vector(word) for word in s1.split() ],axis=0)\n",
        "    return vector_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxlT32GkHWEi"
      },
      "source": [
        "function for weighted average of fasttext word2vec embeddings of words in extracts , weight of word is inversly proportional to its frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULnyCkV1--3J"
      },
      "source": [
        "def fasttext_Invers_freq_wordembedding_method(s1):\n",
        "    a=0.001\n",
        "    weight = [a/(a+frequencies.get(word, 0)/total_freq) for word in s1.split() ]\n",
        "    vector_1 = np.average([ft.get_word_vector(word) for word in s1.split() ],axis=0,weights=weight)\n",
        "    return vector_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2LPXiYBI4h5"
      },
      "source": [
        "function for fasttext no stop words embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJWMNPr-_bbu"
      },
      "source": [
        "def fasttext_no_stop_wordembedding_method(s1):\n",
        "    vector_1 = np.mean([ft.get_word_vector(word) for word in s1 ],axis=0)\n",
        "    return vector_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Do-oXY0K0n5"
      },
      "source": [
        "fasttext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbN26aBZAtqT"
      },
      "source": [
        "corpus_embeddings = []\n",
        "for key in sentences:\n",
        "  y = sentences[key]\n",
        "  corpus_embeddings.append(fasttext_wordembedding_method(y))\n",
        "que_id = []\n",
        "que = []\n",
        "similar_sentences = []\n",
        "scores = []\n",
        "corpus_ids = []\n",
        "for i in range(len(queries)):\n",
        "  query = queries[i]\n",
        "  q_e = fasttext_wordembedding_method(query)\n",
        "  q_e = torch.tensor(q_e)\n",
        "  cos_scores = util.pytorch_cos_sim(q_e, corpus_embeddings)[0]\n",
        "  top_results = torch.topk(cos_scores, k=10)\n",
        "  for score, idx in zip(top_results[0], top_results[1]):\n",
        "        \n",
        "        que.append(query)\n",
        "        similar_sentences.append(corpus[int(idx)])\n",
        "        scores.append(score)\n",
        "        corpus_ids.append(keys[int(idx)])\n",
        "        if i==0:\n",
        "          que_id.append(e1)\n",
        "        if i==1:\n",
        "          que_id.append(e2)\n",
        "        if i==2:\n",
        "          que_id.append(e3)\n",
        "data = {\"query\":que, \"similar_sentences\":similar_sentences , \"scores\":scores , \"corpus_ids\" : corpus_ids}\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('cosine_similarity_fasttext_freq.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acSpxJrBK7IZ"
      },
      "source": [
        "fasttext inverse frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE7EzDNTAZxL"
      },
      "source": [
        "corpus_embeddings = []\n",
        "for key in sentences:\n",
        "  y = sentences[key]\n",
        "  corpus_embeddings.append(fasttext_Invers_freq_wordembedding_method(y))\n",
        "que_id = []\n",
        "que = []\n",
        "similar_sentences = []\n",
        "scores = []\n",
        "corpus_ids = []\n",
        "for i in range(len(queries)):\n",
        "  query = queries[i]\n",
        "  q_e = fasttext_Invers_freq_wordembedding_method(query)\n",
        "  q_e = torch.tensor(q_e)\n",
        "  cos_scores = util.pytorch_cos_sim(q_e, corpus_embeddings)[0]\n",
        "  top_results = torch.topk(cos_scores, k=10)\n",
        "  for score, idx in zip(top_results[0], top_results[1]):\n",
        "        \n",
        "        que.append(query)\n",
        "        similar_sentences.append(corpus[int(idx)])\n",
        "        scores.append(score)\n",
        "        corpus_ids.append(keys[int(idx)])\n",
        "        if i==0:\n",
        "          que_id.append(e1)\n",
        "        if i==1:\n",
        "          que_id.append(e2)\n",
        "        if i==2:\n",
        "          que_id.append(e3)\n",
        "data = {\"query\":que, \"similar_sentences\":similar_sentences , \"scores\":scores , \"corpus_ids\" : corpus_ids}\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('cosine_similarity_fasttext_Invers_freq.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tED6pWwhK-ve"
      },
      "source": [
        "fastext no stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1dD-7MUCcOD"
      },
      "source": [
        "corpus_embeddings = []\n",
        "for y in filtered_sentences:\n",
        "  corpus_embeddings.append(fasttext_no_stop_wordembedding_method(y))\n",
        "que_id = []\n",
        "que = []\n",
        "similar_sentences = []\n",
        "scores = []\n",
        "corpus_ids = []\n",
        "for i in range(len(filtered_quiries)):\n",
        "  query = filtered_quiries[i]\n",
        "  q_e = fasttext_no_stop_wordembedding_method(query)\n",
        "  q_e = torch.tensor(q_e)\n",
        "  cos_scores = util.pytorch_cos_sim(q_e, corpus_embeddings)[0]\n",
        "  top_results = torch.topk(cos_scores, k=10)\n",
        "  for score, idx in zip(top_results[0], top_results[1]):\n",
        "        \n",
        "        que.append(query)\n",
        "        similar_sentences.append(corpus[int(idx)])\n",
        "        scores.append(score)\n",
        "        corpus_ids.append(keys[int(idx)])\n",
        "        if i==0:\n",
        "          que_id.append(e1)\n",
        "        if i==1:\n",
        "          que_id.append(e2)\n",
        "        if i==2:\n",
        "          que_id.append(e3)\n",
        "data = {\"query\":que, \"similar_sentences\":similar_sentences , \"scores\":scores , \"corpus_ids\" : corpus_ids}\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('cosine_similarity_fasttext_no_stop_words.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbQuF37paZLh"
      },
      "source": [
        "# !pip install simple_elmo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDliwLJcarJI"
      },
      "source": [
        "# from simple_elmo import ElmoModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PMAFX_lawaa"
      },
      "source": [
        "# model = ElmoModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JdJzPFca1AQ"
      },
      "source": [
        "# model.load(\"193.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emthUz7vhuoW"
      },
      "source": [
        "# corpus_embeddings = model.get_elmo_vector_average([sentences[key] for key in sentences])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}